{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Centric ML Development using Snowflake and Amazon SageMaker\n",
    "\n",
    "This notebook guides you through a Data Centric machine learning (ML) development process using Snowflake and Amazon SageMaker. We demonstrate the use case through a credit-risk analysis use case.\n",
    "\n",
    "What you will learn:\n",
    "\n",
    "* How to use the Snowflake connector for Amazon SageMaker DataWrangler.\n",
    "* How to train a model using a AWS Marketplace algorithm, Autogluon.\n",
    "* How to enrich your ML dataset with data from the Snowflake Data Marketplace.\n",
    "* How to iterate on your model design and data prep flows with the provided tools.\n",
    "* How to deploy a production scoring pipeline.\n",
    "\n",
    "How to run it:\n",
    "\n",
    "This notebook was designed for Amazon SageMaker Studio, and should ran in the **Python 3 (Data Science)** kernel.\n",
    "\n",
    "**Have feedback?** <br>\n",
    "Contact: [Dylan Tong](mailto:dylatong@amazon.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Pre-requisites\n",
    "\n",
    "1. **Setup Environment** using the provided CloudFormation [templates](https://github.com/dylan-tong-aws/snowflake-sagemaker-workshops). You can use the following launch button if you haven't completed this step yet.\n",
    "\n",
    "    <a href=\"https://console.aws.amazon.com/cloudformation/home?region=region#/stacks/new?stackName=kernel-builder&templateURL=https://dtong-public-fileshare.s3.us-west-2.amazonaws.com/snowflake-sagemaker-workshop/src/deploy/cf/workshop-setup-w-studio.yml\"/> ![Existing SageMaker Studio Environment](./images/deploy-to-aws.png) \n",
    "    \n",
    "    The template will create IAM and S3 resources simplifying the setup.\n",
    "\n",
    "2. Create a [Storage Integration](https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html) resource in Snowflake:\n",
    "\n",
    "    >CREATE STORAGE INTEGRATION IF NOT EXISTS SAGEMAKER_DATAWRANGLER_INTEGRATION </br>\n",
    "      TYPE = EXTERNAL_STAGE </br>\n",
    "      STORAGE_PROVIDER = S3 </br>\n",
    "      STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::(YOUR_AWS_ACCOUNT_ID):role/snowflake-datawrangler-role' </br>\n",
    "      ENABLED = true </br>\n",
    "      STORAGE_ALLOWED_LOCATIONS = ('s3://snowflake-sagemaker-(YOUR_AWS_REGION)-(YOUR_AWS_ACCOUNT_ID)/') </br>\n",
    "      \n",
    "     Replace (YOUR_AWS_REGION) and (YOUR_AWS_ACCOUNT_ID) with your AWS account ID and the region. \n",
    "     \n",
    "     \n",
    "3. Create an S3 [External Stage](https://docs.snowflake.com/en/sql-reference/sql/create-stage.html): \n",
    "\n",
    "    >GRANT CREATE STAGE ON SCHEMA public to ROLE sagemaker_role; </br>\n",
    "    GRANT USAGE ON INTEGRATION SAGEMAKER_DATAWRANGLER_INTEGRATION TO ROLE sagemaker_role; </br>\n",
    "    \n",
    "    >CREATE OR REPLACE STAGE datawrangler_stage </br>\n",
    "      STORAGE_INTEGRATION = SAGEMAKER_DATAWRANGLER_INTEGRATION </br>\n",
    "      URL = 's3://snowflake-sagemaker-(YOUR_AWS_REGION)-(YOUR_AWS_ACCOUNT_ID)/' </br>\n",
    "     \n",
    "   Replace (YOUR_AWS_REGION) and (YOUR_AWS_ACCOUNT_ID) with your AWS account ID and the region.\n",
    "    \n",
    "     \n",
    "4. Run \n",
    "\n",
    "    >DESC INTEGRATION SAGEMAKER_DATAWRANGLER_INTEGRATION\n",
    "\n",
    "    - Copy the property value for STORAGE_AWS_IAM_USER_ARN. It will look similar to arn:aws:iam::123456789:user/mgm4-s-ssxxxxx\n",
    "    - Copy the property value for STORAGE_AWS_EXTERNAL_ID. It will look similar to ACCOUNT_SFCRole=2_6hy7rHI61mnORWhxlhTa7LcS3X4=  \n",
    "    \n",
    "    \n",
    "5. Run the following cell to install the notebook's dependencies. (TODO: consider options to avoid this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.7/site-packages (7.5.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: scikit-plot in /opt/conda/lib/python3.7/site-packages (0.3.7)\n",
      "Requirement already satisfied: snowflake-connector-python[pandas] in /opt/conda/lib/python3.7/site-packages (2.4.6)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (5.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (5.0.4)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (7.12.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.4)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (2.5.2)\n",
      "Requirement already satisfied: pexpect in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (56.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.4.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: parso>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets) (0.5.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.15.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.1.8)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (18.1.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /opt/conda/lib/python3.7/site-packages (from seaborn) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.7/site-packages (from scikit-plot) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.10 in /opt/conda/lib/python3.7/site-packages (from scikit-plot) (0.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.3.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect->ipython>=4.0.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.8)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.3.0)\n",
      "Requirement already satisfied: azure-common<2.0.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.1.27)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.4.4 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.17.74)\n",
      "Requirement already satisfied: cryptography<4.0.0,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.4.7)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.14.5)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2020.12.5)\n",
      "Requirement already satisfied: pyOpenSSL<21.0.0,>=16.2.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (19.1.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.10.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (2.25.1)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.0.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (12.8.1)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (1.2.1)\n",
      "Requirement already satisfied: pyarrow<3.1.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from snowflake-connector-python[pandas]) (3.0.0)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /opt/conda/lib/python3.7/site-packages (from azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python[pandas]) (0.6.21)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python[pandas]) (1.16.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0.0,>=1.4.4->snowflake-connector-python[pandas]) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.74 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0.0,>=1.4.4->snowflake-connector-python[pandas]) (1.20.74)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0.0,>=1.4.4->snowflake-connector-python[pandas]) (0.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.74->boto3<2.0.0,>=1.4.4->snowflake-connector-python[pandas]) (1.26.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.19)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from msrest>=0.6.18->azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python[pandas]) (1.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from msrest>=0.6.18->azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python[pandas]) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python[pandas]) (3.1.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets seaborn scikit-plot snowflake-connector-python[pandas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run the following cells to import the required libraries and set the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import AlgorithmEstimator, get_execution_role\n",
    "\n",
    "import utils.algo\n",
    "import utils.dw\n",
    "from workflow.pipeline import BlueprintFactory\n",
    "from utils.trust import ModelInspector\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region     = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket     = f\"snowflake-sagemaker-{region}-{account_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Configure Permissions\n",
    "---\n",
    "\n",
    "#### 1.1 Provide Snowflake Access to External Stage\n",
    "\n",
    "You have already associated your Snowflake storage integration to an IAM role. We need to configure that IAM role with the proper trust policy, so that Snowflake can write data into the external stage. **Run the following cell** to generate a direct link to the IAM role.  \n",
    "    \n",
    "A placeholder trust policy was created by the CloudFormation template. It looks as follows:\n",
    "\n",
    "            {\n",
    "              \"Version\": \"2012-10-17\",\n",
    "              \"Statement\": [\n",
    "                {\n",
    "                  \"Effect\": \"Allow\",\n",
    "                  \"Principal\": {\n",
    "                    \"AWS\": \"arn:aws:iam::YOUR_ACCOUNT_ID:root\"\n",
    "                  },\n",
    "                  \"Action\": \"sts:AssumeRole\",\n",
    "                  \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                      \"sts:ExternalId\": \"RUN_DESC_INTEGRATION_TO_GET_EXTERNALID\"\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "\n",
    "    \n",
    "Update the policy with property values that you obtained from the DESC INTEGRATION command.\n",
    "    \n",
    "    - Replace \"arn:aws:iam::YOUR_ACCOUNT_ID:root\" with the value obtained for STORAGE_AWS_IAM_USER_ARN\n",
    "    - Replace \"RUN_DESC_INTEGRATION_TO_GET_EXTERNALID\" with the value obtained for STORAGE_AWS_EXTERNAL_ID\n",
    "\n",
    "Proceed by following the generated link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "IAM console redirect: https://console.aws.amazon.com/iam/home?#/roles/snowflake-datawrangler-role-us-west-1?section=trust"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iam_trust_url = f\"https://console.aws.amazon.com/iam/home?#/roles/snowflake-datawrangler-role-{region}?section=trust\"\n",
    "md(f\"IAM console redirect: {iam_trust_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1.2 Provide Access to [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/)\n",
    "\n",
    "Later in this lab, you will need to provide database credentials. We need to provide your Amazon SageMaker Studio environment permission to access AWS Secrets Manager so that the credentials are stored securely. \n",
    "\n",
    "We do this by **attaching the [SecretsManagerReadWrite](https://console.aws.amazon.com/iam/home?#/policies/arn:aws:iam::aws:policy/SecretsManagerReadWrite$jsonEditor) managed policy** to your Amazon SageMaker Studio's execution role. The authentication method used in this lab requires your Amazon SageMaker environment to have access to [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/). If you provisioned this Amazon SageMaker Studio environment using the provided CloudFormation templates, this step has already been done for you.\n",
    "\n",
    "If not, run the following cell to generate a direct link to the IAM role and attach the managed policy. Your execution role should look like the following:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/secret-manager-policy.png\" align=\"left\" width=\"65%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "IAM console redirect: https://console.aws.amazon.com/iam/home?#/roles/cf-sm-studio-role-us-west-1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolename = get_execution_role().split(\"/\")[-1]\n",
    "exec_role_url = f\"https://console.aws.amazon.com/iam/home?#/roles/{rolename}\"\n",
    "md(f\"IAM console redirect: {exec_role_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build your Data Prep Flow\n",
    "---\n",
    "\n",
    "Next, we'll use the Snowflake connector for Data Wrangler to access our data and describe a pipeline that prepares our data for machine learning (ML) training. \n",
    "\n",
    "#### 2.1 Create a new Data Wrangler Flow\n",
    "\n",
    "<img src=\"./images/create-data-flow.png\" width=30% align=\"left\" img/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.2. Create a Snowflake Connection\n",
    "\n",
    "Select Snowflake from the data source dropdown. </br>\n",
    "\n",
    "<img src=\"./images/create-snowflake-connection.png\" width=75% align=\"left\" img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Configure the connection with information about your account and storage integration. </br>\n",
    "\n",
    "\n",
    "<img src=\"./images/configure-snowflake-connection.png\" width=45% align=\"left\" img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.3 Explore your Snowflake Data\n",
    "\n",
    "1. Select your data warehouse, database and schema. \n",
    "2. Run SELECT * FROM ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML. The data set will be sampled by default.\n",
    "\n",
    "This data set consists of a year of loan data from the [LendingClub](https://www.lendingclub.com/). It has been augmented with unemployment rate data provided by [Knoema](https://knoema.com/) from the [Snowflake Data Marketplace](https://www.snowflake.com/data-marketplace/). In this lab, we will demonstrate the data centric approach to improving machine learning model by showing how data from the Snowflake Marketplace can improve prediction performance. \n",
    "\n",
    "<img src=\"./images/query-and-explore-snowflake.png\" width=80% align=\"left\" img/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.4 Refine your Features\n",
    "\n",
    "With a bit of intuition and experience, you should be able to quickly spot some data columns that are unlikely to be good features. For instance, **LOAN_ID** is the unique identifier, and intuitively, we know that it has no meaningful correlation with loan defaults. On the other hand, **LOAN_AMNT** (*loan amount*) and **GRADE**, has potential. Large loans might bear greater risk. Similarly, we expect Grade F loans be riskier than Grade A ones. Thus, the former would help predict defaults. The machine learning algorithms can learn from these patterns and build a model that can predict the risk of defaults.\n",
    "\n",
    "**Run** the following query to acquire a filtered list of potential features. Next, click the **Import** button and name your training dataset. \n",
    "\n",
    "Note: \n",
    "- You also have the alternative option to drop columns as part of your DataWrangler data prep flow.\n",
    "- We're using Snowflake's sampling functionality to create a train/test set split. This query generates a repeatable 80% sampling of our data.\n",
    "\n",
    "**SELECT** </br>\n",
    "   >LOAN_ID, </br>\n",
    "    LOAN_AMNT, </br> \n",
    "    FUNDED_AMNT, </br>\n",
    "    TERM, </br>\n",
    "    INT_RATE, </br>\n",
    "    INSTALLMENT, </br>\n",
    "    GRADE, </br>\n",
    "    SUB_GRADE, </br>\n",
    "    EMP_LENGTH, </br>\n",
    "    HOME_OWNERSHIP, </br>\n",
    "    ANNUAL_INC, </br>\n",
    "    VERIFICATION_STATUS, </br>\n",
    "    PYMNT_PLAN, </br>\n",
    "    PURPOSE, </br>\n",
    "    ZIP_SCODE, </br>\n",
    "    DTI, </br>\n",
    "    DELINQ_2YRS, </br>\n",
    "    EARLIEST_CR_LINE, </br>\n",
    "    INQ_LAST_6MON, </br>\n",
    "    MNTHS_SINCE_LAST_DELINQ, </br>\n",
    "    MNTHS_SINCE_LAST_RECORD, </br>\n",
    "    OPEN_ACC, </br>\n",
    "    PUB_REC, </br>\n",
    "    REVOL_BAL, </br>\n",
    "    REVOL_UTIL, </br>\n",
    "    TOTAL_ACC, </br>\n",
    "    INITIAL_LIST_STATUS, </br>\n",
    "    MTHS_SINCE_LAST_MAJOR_DEROG, </br>\n",
    "    POLICY_CODE, </br>\n",
    "    LOAN_DEFAULT, </br>\n",
    "    ISSUE_MONTH </br>\n",
    "    \n",
    "**FROM** ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML </br>\n",
    "sample block (80) REPEATABLE(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.5 Profile your Data\n",
    "\n",
    "Data profiling and analysis is often a good place to start before you begin building your data preparation flow. Follow the steps illustrated by the video to create a histogram to analyze the distribution of loan defaults. **LOAN_DEFAULT** is the feature to plot.\n",
    "\n",
    "![Loan Default Distribution](./images/create-histogram.gif)\n",
    "\n",
    "\n",
    "Typical of loan default data, the dataset is skew. There are less default cases than successful ones. Our analysis helps us confirm that the skew is manageable. An AutoML algorithm will apply the appropriate mitigation techniques for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.6 Apply Feature Transforms\n",
    "\n",
    "There are features that require transformations before the data can be trained. Later on, you will use an AutoML algorithm to train a model. The algorithm automates a great deal of feature engineering. Nonetheless, there is some data preparation that cannot be automated. We will go through the exercise of using SageMaker DataWrangler to transform the **INT_RATE** column. \n",
    "\n",
    "First, select the tail of the flow and select **Add transform**. </br>\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=./images/create-transform.png width=\"60%\" align=\"left\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INT_RATE** is an example of column that requires human input to properly process. This column is stored as a string type. ML algorithms only on numerical data. AutoML algorithms typically provide automation. They will detect string type columns and convert them accordingly. However, generally, they will assume that this column is categorical and apply [one-hot encoding](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-cat-encode). This is the incorrect transformation and will render this feature useless. Instead, this feature should be treated as a continuous numerical feature. \n",
    "\n",
    "**TERM**, on the other hand, is seemingly similar but it could be left as a string. The dataset consists of 36 and 60 month terms. If you leave it as is, an AutoML algorithm will automically one-hot encode this column.\n",
    "\n",
    "Use the **\"Search and Edit\"** transform to remove the \"%\" sign from **INT_RATE** so that we can convert the column into numerical values.\n",
    "\n",
    "</br>\n",
    "\n",
    "<img src=./images/ft-search-replace.png width=\"70%\" align=\"left\"/>\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the **\"Parse column as type\"** to convert the data column from **String** to **Float**.\n",
    "\n",
    "\n",
    "<img src=./images/ft-parse-type.png width=\"70%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The **VERIFICATION_STATUS** column has a minor data quality issue. This feature is effectively a boolean, but the verified status is represented as two values. This transformation requires custom logic. In such cases, we can run a custom script using a **Custom Transform**. \n",
    "\n",
    "Copy the following PySpark script:\n",
    "\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import LongType\n",
    "\n",
    "    def categories(status) : \n",
    "      if not status :\n",
    "        return None\n",
    "      elif status == \"not verified\" :    \n",
    "        return 0\n",
    "      elif status == \"VERIFIED - income\":\n",
    "        return 1\n",
    "      elif status == \"VERIFIED - income source\":\n",
    "        return 1\n",
    "      else :\n",
    "        return None\n",
    "\n",
    "    bucket_udf = udf(categories, LongType()) \n",
    "    df = df.withColumn(\"VERFIED\", bucket_udf(\"VERIFICATION_STATUS\"))\n",
    "\n",
    "Apply the script by creating a **Custom Transform** as shown below:\n",
    "\n",
    "<img src=./images/ft-custom-transform.png width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we created a new column we need to delete the source. Select **Manage columns** and apply the following settings:\n",
    "\n",
    "**Transform:** Drop column </br>\n",
    "**Column to Drop:** VERIFICATION_STATUS </br>\n",
    "\n",
    "\n",
    "<img src=./images/drop-column-vstatus.png width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Finally, drop the **LOAN_ID** column. This is a unique identifier for each loan. It will only add noise to the training data. Select **Manage columns** and apply the following settings:\n",
    "\n",
    "**Transform:** Drop column </br>\n",
    "**Column to Drop:** LOAN_ID </br>\n",
    "\n",
    "<img src=\"images/ft-drop-loanid.png\" width=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Click on **\"back to data flow\"**. You should see the five transforms steps at the tail of your data prep flow. \n",
    "\n",
    "<img src=./images/flow-w-transforms.png width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2.7 Data Validation\n",
    "\n",
    "It is best practice to perform data validation before model training. DataWrangler provides useful reports to faciliate data bias and target leakage analysis. Our use case, loan default prediction, has legal and ethical risk considerations. For instance, [data bias](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html#data-wrangler-bias-report) can result in models that could put certain demographics at a disadvantage. For instance, a loan default model could unfairly reject a disproportionate number of minority group loan applications without merit as a consequence of training on flawed data.\n",
    "\n",
    "You also want to avoid [target leakage](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html#data-wrangler-analysis-target-leakage). Target leakage occurs when you accidently train a model with features that are not available in production. As a consequence, you end up with a deceptively effective model in development that causes problems in production. You can mitigate production issues by performing target leakage analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a Target Leakage report as demonstrated by the video below. Use the following settings:\n",
    "\n",
    "- **Max features:** 30\n",
    "- **Problem Type:** Classification\n",
    "- **Target:** LOAN_DEFAULT\n",
    "\n",
    "\n",
    "![Target Leakage Report](./images/target-leakage-report.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The report indicates that there is no target leakage risk. It does detect some potentially redundant features. The AutoML algorithm that you will use will mitigate redundant features. As an optional exercise, you can run experiments and determine whether these potentially redundant features effect model performance.\n",
    "\n",
    "<img src=./images/target-leakage-results.png width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Create a Bias Report as demonstrated by the following video. Use the following settings:\n",
    "\n",
    "- **Select the column your model predicts (target):** LOAN_DEFAULT\n",
    "- **Is your predicted column a value or threshold?:** Value\n",
    "- **Predicted value(s):** 0;1\n",
    "- **Select the column to analyze for bias:** ZIPS_CODE\n",
    "- **Is your column a value or threshold?:** Value\n",
    "- **Column value(s) to analyze for bias:** 200xx;207xx;206xx;900xx;100xx;941xx\n",
    "\n",
    "![Bias Report](./images/create-bias-report.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data does not have any obvious sensitive attributes like gender and race. However, it does contain zip codes. It's possible that we have a flawed dataset with an abnormal number of loan defaults in minority communities. This might not represent the actual distribution. Regardless, this situation could create a model that is biased against minorities resulting in legal risk.\n",
    "\n",
    "The report does not reveal any salient data bias issues.\n",
    "\n",
    "<img src=./images/bias-report-results.png width=\"65%\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Prototype your Model\n",
    "\n",
    "Amazon SageMaker provides a broad range of remote training services that can help you scale your ML experimentation, training and tuning process. But before you commit to a long running process, it maybe desireable to explore different combinations of candidate features and be able to rapidly iterate on a few prototypes.  \n",
    "\n",
    "#### 3.1 Create a Quick Model Report\n",
    "\n",
    "Amazon Data Wrangler provides a **[Quick Model](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.html#data-wrangler-quick-model)** report which can serve as a prototyping mechanism. The report will sample your dataset, process your flow and generates a Random Forest Model. The report provides model and feature importance scores to help you assess:\n",
    "\n",
    "* What features are most impactful?\n",
    "* Does your data have enough predictive signals to produce a practical model?\n",
    "* Are your changes to your dataset leading to improvements?\n",
    "\n",
    "Navigate to the Analysis panal from the tail end of your flowâ€”as you did in the previous section. \n",
    "\n",
    "Configure your report:\n",
    "* **Analysis type:** Quick Model\n",
    "* **Analysis name:** Quick Test\n",
    "* **Label:** LOAN_DEFAULT\n",
    "\n",
    "It will take about 5 minutes to generate a report like the following:\n",
    "\n",
    "<img src=\"./images/quick-model-iter1.png\" width=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of the feature importance ranking in the bar chart. This gives you an *approximation* of which features have strong predictive signals. The F1 score of <span style=\"color:yellow\">**0.691**</span> is not great. However, you can expect better results with a complete training and tuning process. The score tells you that your dataset has potential to produce a practical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Iterate, Experiment and Improve\n",
    "\n",
    "You can improve your model's performance through further feature engineering and improvements to your dataset. Next, you will do just that by enriching your dataset with data obtained from the Snowflake's Data Marketplace.\n",
    "\n",
    "In the following sections, we'll be modifying our existing flow. In practice, you should version control your .flow files first through the [Git integration](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-git-repo.html)\n",
    "\n",
    "#### Step 4.1 Explore and Extract Candidate Features from the Data Marketplace\n",
    "\n",
    "Add a new data source to your existing flow. Select the **Import** sub tab and click on the Snowflake icon. Run the following query to extract the unemployment rate data that you obtained from the [Snowflake Data Marketplace](https://www.snowflake.com/data-marketplace/).\n",
    "\n",
    "**SELECT**\n",
    ">LOAN_ID, </br>\n",
    "UNEMPLOYMENT_RATE </br>\n",
    "\n",
    "**FROM** ML_LENDER_DATA.ML_DATA.UNEMPLOYMENT_DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Step 4.2 Augment your Dataset\n",
    "\n",
    "Next, you're going to merge the two datasets. There are many ways to do this. You could have perform this entirely using Snowflake. In this lab, you'll learn how to perform this merge through DataWrangler. This method provides you with a visualization of the modified flow and the change can be version control within your Git repository.\n",
    "\n",
    "\n",
    "First, **Delete** the last transformation from the original flow, so that we have **LOAN_ID** available in the original dataset.\n",
    "\n",
    "![Delete Step](./images/delete-step.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, replicate the steps in the following video to merge the unemployment rate feature into your dataset.\n",
    "\n",
    "1. Click on the end of the original flow and select the **Join** operator.\n",
    "2. Select the other flow.\n",
    "3. Select **Left Outer** as the **Join Type**.\n",
    "4. Select **LOAN_ID** for both the **Left** and **Right** join keys.\n",
    "\n",
    "![Join Datasets](./images/join-and-enrich-flow.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, discard the join keys. Same as before, use the **Manage columns** transform to drop columns. \n",
    "\n",
    "1. Select the Join node and **Add transform**.\n",
    "2. Drop the columns, **LOAN_ID_0** and **LOAN_ID_1**.\n",
    "\n",
    "<img src=\"./images/ft-drop-loanid0.png\" width=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ft-drop-loanid1.png\" width=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Step 4.3 Re-validate your Dataset\n",
    "\n",
    "You should re-validate your dataset since it has been modified. The Target Leakage report calculates the correlation between your features and the target variable. In effect, it provides you with an idea of how likely your new feature will improve your model. The report should present the new feature, **UNEMPLOYMENT_RATE**, as the feature with the highest predictive potential.\n",
    "\n",
    "<img src=\"./images/target-leakage-report-w-unemployment.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4.4 Evaluate your Dataset Modifications\n",
    "\n",
    "Next, we're going to evaluate whether our new feature is beneficial. We will use the Quick Model report again to get a quick assessment. Note that in practice, you might want to be more thorough and fully train and tune a model on some your dataset iterations so that you have a reliable baseline. For the sake of demonstration, we use the Quick Model report exclusively.\n",
    "\n",
    "Create a new **Quick Model** report to assess the impact of your modifications. The results should be similiar to the following:\n",
    "\n",
    "<img src=\"./images/quick-model-iter2.png\" width=\"65%\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of key takeaways:\n",
    "* UNEMPLOYMENT_RATE is clearly ranked as the most important feature. \n",
    "* The F1 score increased to <span style=\"color:lightgreen\">**0.784**</span> from 0.691.\n",
    "\n",
    "This tells us that we are likely heading in the right direction. We added a feature that generated noteable improvements to the \"quick model\" and the new feature had the greatest impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Generate your Dataset\n",
    "\n",
    "We are now ready to fully train and tune a model. First, we need to generate our datasets by executing the data flow that we've created.\n",
    "\n",
    "#### 5.1 Export Your Data Flow\n",
    "\n",
    "DataWrangler supports multiple ways to [export](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-export.html) the flow for execution. In this lab, you will select the option that generates a notebook that can be run to execute the flow as a [SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) job. This is the simplest option. The other options offer capabilities that you might value in production deployments. Follow steps as demonstrated in the following video.\n",
    "\n",
    "\n",
    "![Export Script](./images/data-flow-export.gif)\n",
    "\n",
    "\n",
    "---\n",
    "#### 5.2 Execute the Data Flow\n",
    "\n",
    "Follow the steps outlined in the generated notebook. Run the cells and wait for the processing job to complete. Copy the output S3 URI of the processed dataset. The S3 URI will look similar to: *s3://(YOUR BUCKET)/export-flow-23-23-17-34-6a8a80ec/output/data-wrangler-flow-processing-23-23-17-34-6a8a80ec*. \n",
    "\n",
    "Set the variable **PREP_DATA_S3** in the following cell to that S3 URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREP_DATA_S3 = \"s3://sagemaker-us-west-1-407247006381/export-flow-30-21-26-10-7e5b81a7/output/data-wrangler-flow-processing-30-21-26-10-7e5b81a7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Train Your Model\n",
    "\n",
    "#### 6.1 Subscribe to AutoGluon in the AWS Marketplace\n",
    "\n",
    "Next, you are going to subscribe to the AutoGluon Marketplace algorithm. This provides your account access to a SageMaker compatible container for running AutoGluon. This Marketplace algorithm is managed by AWS and doesn't have additonal software costs. Marketplace algorithms are similar to SageMaker built-in algorithms. Once subscribed, you can run the algorithm to train and serve models with \"low-to-no-code\". \n",
    "\n",
    "Follow these steps to subscribe to the AWS Marketplace AutoGluon algorithm:\n",
    "\n",
    "1. Click **[this URL](https://aws.amazon.com/marketplace/pp/Amazon-Web-Services-AutoGluon-Tabular/prodview-n4zf5pmjt7ism)** to navigate to the AutoGluon product page.\n",
    "2. Select the orange \"Continue to Subscribe\" button.\n",
    "3. Run the helper function below to identify the AWS resource ID (ARN) of your AutoGluon Marketplace algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tabular AutoGluon ARN in your region is arn:aws:sagemaker:us-west-1:382657785993:algorithm/autogluon-tabular-v3-5-cb7001bd0e8243b50adc3338deb44a48.\n"
     ]
    }
   ],
   "source": [
    "AUTOGLUON_PRODUCT = \"autogluon-tabular-v3-5-cb7001bd0e8243b50adc3338deb44a48\"\n",
    "algorithm_arn = utils.algo.get_algorithm_arn(region, AUTOGLUON_PRODUCT)\n",
    "print(\"The Tabular AutoGluon ARN in your region is {}.\".format(algorithm_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll configure our algorithm for remote training (Note: you can configure and launch the job using the AWS console as an alternative to the SDK).\n",
    "\n",
    "1. **Hyperparamters**: AutoML algorithms like AutoGluon are designed to automate hyperparameter tuning using hyperparamter search algorithms like Bayesian Optimization. Thus, setting hyperparameters are optional. However, you can override the defaults. We'll use the default configurations in this lab, so we only need to identify the name of the target label column. The other configurations are commented out and serve as examples.\n",
    "\n",
    "2. **Infrastructure**: We're using SageMaker's remote training service, so we need to specify the infrastructure to allocate. Since we're using a Marketplace product, we need to be aware of the subset of supported instances. \n",
    "\n",
    "3. **Data**: lastly, we need to identify the location of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    " \"init_args\":{\n",
    "    \"label\": \"LOAN_DEFAULT\"\n",
    " }\n",
    "}\n",
    "\n",
    "data_uri = utils.dw.get_data_uri(PREP_DATA_S3)\n",
    "compatible_training_instance_type='ml.m5.4xlarge' \n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=data_uri, content_type='csv')\n",
    "\n",
    "autogluon_model = AlgorithmEstimator(algorithm_arn=algorithm_arn, \n",
    "                              role=role, \n",
    "                              instance_count=1, \n",
    "                              instance_type=compatible_training_instance_type, \n",
    "                              sagemaker_session=sess, \n",
    "                              base_job_name='autogluon',\n",
    "                              hyperparameters=hyperparameters,\n",
    "                              train_volume_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the next cell will launch the remote training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-30 21:33:04 Starting - Starting the training job...\n",
      "2021-07-30 21:33:29 Starting - Launching requested ML instancesProfilerReport-1627680784: InProgress\n",
      "...\n",
      "2021-07-30 21:33:58 Starting - Preparing the instances for training.........\n",
      "2021-07-30 21:35:30 Downloading - Downloading input data\n",
      "2021-07-30 21:35:30 Training - Downloading the training image.........\n",
      "2021-07-30 21:37:01 Training - Training image download completed. Training in progress..\u001b[34m2021-07-30 21:37:01,793 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,795 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,795 sagemaker-training-toolkit INFO     Failed to parse hyperparameter init_args value {'label': 'LOAN_DEFAULT'} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,795 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fit_args value {'presets': ['optimize_for_deployment']} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,795 sagemaker-training-toolkit INFO     Failed to parse hyperparameter feature_importance value False to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,807 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"feature_importance\":\"False\",\"fit_args\":\"{\\'presets\\': [\\'optimize_for_deployment\\']}\",\"init_args\":\"{\\'label\\': \\'LOAN_DEFAULT\\'}\"}', 'SM_USER_ENTRY_POINT': 'train.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'train', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '16', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': '/opt/ml/code', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"feature_importance\":\"False\",\"fit_args\":\"{\\'presets\\': [\\'optimize_for_deployment\\']}\",\"init_args\":\"{\\'label\\': \\'LOAN_DEFAULT\\'}\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"autogluon-2021-07-30-21-33-04-033\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}', 'SM_USER_ARGS': '[\"--feature_importance\",\"False\",\"--fit_args\",\"{\\'presets\\': [\\'optimize_for_deployment\\']}\",\"--init_args\",\"{\\'label\\': \\'LOAN_DEFAULT\\'}\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_INIT_ARGS': \"{'label': 'LOAN_DEFAULT'}\", 'SM_HP_FIT_ARGS': \"{'presets': ['optimize_for_deployment']}\", 'SM_HP_FEATURE_IMPORTANCE': 'False'}\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,809 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,809 sagemaker-training-toolkit INFO     Failed to parse hyperparameter init_args value {'label': 'LOAN_DEFAULT'} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,809 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fit_args value {'presets': ['optimize_for_deployment']} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:01,809 sagemaker-training-toolkit INFO     Failed to parse hyperparameter feature_importance value False to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,843 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,844 sagemaker-training-toolkit INFO     Failed to parse hyperparameter init_args value {'label': 'LOAN_DEFAULT'} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,844 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fit_args value {'presets': ['optimize_for_deployment']} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,844 sagemaker-training-toolkit INFO     Failed to parse hyperparameter feature_importance value False to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,857 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,857 sagemaker-training-toolkit INFO     Failed to parse hyperparameter init_args value {'label': 'LOAN_DEFAULT'} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,857 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fit_args value {'presets': ['optimize_for_deployment']} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,857 sagemaker-training-toolkit INFO     Failed to parse hyperparameter feature_importance value False to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-07-30 21:37:04,868 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"init_args\": \"{'label': 'LOAN_DEFAULT'}\",\n",
      "        \"fit_args\": \"{'presets': ['optimize_for_deployment']}\",\n",
      "        \"feature_importance\": \"False\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"autogluon-2021-07-30-21-33-04-033\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"feature_importance\":\"False\",\"fit_args\":\"{'presets': ['optimize_for_deployment']}\",\"init_args\":\"{'label': 'LOAN_DEFAULT'}\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"feature_importance\":\"False\",\"fit_args\":\"{'presets': ['optimize_for_deployment']}\",\"init_args\":\"{'label': 'LOAN_DEFAULT'}\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"autogluon-2021-07-30-21-33-04-033\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--feature_importance\",\"False\",\"--fit_args\",\"{'presets': ['optimize_for_deployment']}\",\"--init_args\",\"{'label': 'LOAN_DEFAULT'}\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_INIT_ARGS={'label': 'LOAN_DEFAULT'}\u001b[0m\n",
      "\u001b[34mSM_HP_FIT_ARGS={'presets': ['optimize_for_deployment']}\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_IMPORTANCE=False\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 train.py --feature_importance False --fit_args {'presets': ['optimize_for_deployment']} --init_args {'label': 'LOAN_DEFAULT'}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/ml/input:\u001b[0m\n",
      "\u001b[34mtotal 8\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 2 root root 4096 Jul 30 21:35 config\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 3 root root 4096 Jul 30 21:35 data\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/ml/input/config:\u001b[0m\n",
      "\u001b[34mtotal 32\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  124 Jul 30 21:35 hyperparameters.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  681 Jul 30 21:35 init-config.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  127 Jul 30 21:35 inputdataconfig.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root   80 Jul 30 21:35 metric-definition-regex.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  195 Jul 30 21:35 profilerconfig.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root   81 Jul 30 21:35 resourceconfig.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root 1621 Jul 30 21:35 trainingjobconfig.json\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root    2 Jul 30 21:35 upstreamoutputdataconfig.json\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data:\u001b[0m\n",
      "\u001b[34mtotal 8\u001b[0m\n",
      "\u001b[34mdrwxr-xr-x 2 root root 4096 Jul 30 21:35 training\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root  424 Jul 30 21:35 training-manifest\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training:\u001b[0m\n",
      "\u001b[34mtotal 1244\u001b[0m\n",
      "\u001b[34m-rw-r--r-- 1 root root 1270573 Jul 30 21:35 part-00000-b59c38d1-9a67-4d69-aaa7-5d8f3656973a-c000.csv\u001b[0m\n",
      "\u001b[34mINFO:root:0\u001b[0m\n",
      "\u001b[34mIPython could not be loaded!\u001b[0m\n",
      "\u001b[34mDEBUG:asyncio:Using selector: EpollSelector\u001b[0m\n",
      "\u001b[34mfit_args:\u001b[0m\n",
      "\u001b[34mpresets,  type: <class 'list'>,  value: ['optimize_for_deployment']\u001b[0m\n",
      "\u001b[34mTrain files: ['part-00000-b59c38d1-9a67-4d69-aaa7-5d8f3656973a-c000.csv']\u001b[0m\n",
      "\u001b[34mWARNING:autogluon.core.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/opt/ml/model\"\u001b[0m\n",
      "\u001b[34mINFO:root:Presets specified: ['optimize_for_deployment']\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"/opt/ml/model/\"\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.1.0\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:Train Data Rows:    7226\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:Train Data Columns: 30\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\u001b[0m\n",
      "\u001b[34mLevel 25:autogluon.core.utils.utils:AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.utils.utils:#0112 unique label values:  [0.0, 1.0]\u001b[0m\n",
      "\u001b[34mLevel 25:autogluon.core.utils.utils:#011If 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\u001b[0m\n",
      "\u001b[34mINFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "\u001b[34mINFO:numexpr.utils:NumExpr defaulting to 8 threads.\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.data.label_cleaner:Selected class <--> label mapping:  class 1 = 1, class 0 = 0\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Available Memory:                    63567.74 MB\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Train Data (Original)  Memory Usage: 5.67 MB (0.0% of available memory)\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Stage 1 Generators:\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011Fitting AsTypeFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Stage 2 Generators:\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011Fitting FillNaFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Stage 3 Generators:\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011Fitting IdentityFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011Fitting CategoryFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011#011Fitting CategoryMemoryMinimizeFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011Fitting DatetimeFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Stage 4 Generators:\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011Fitting DropUniqueFeatureGenerator...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Useless Original Features (Count: 3): ['INITIAL_LIST_STATUS', 'MTHS_SINCE_LAST_MAJOR_DEROG', 'POLICY_CODE']\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011These features carry no predictive signal and should be manually investigated.\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011This is typically a feature which has the same value for all rows.\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011#011These features do not need to be present at inference time.\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Types of features in original data (raw dtype, special dtypes):\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('float', [])                      : 17 | ['INT_RATE', 'MNTHS_SINCE_LAST_DELINQ', 'MNTHS_SINCE_LAST_RECORD', 'LOAN_AMNT', 'FUNDED_AMNT', ...]\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('int', [])                        :  1 | ['VERFIED']\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('object', [])                     :  8 | ['TERM', 'GRADE', 'SUB_GRADE', 'EMP_LENGTH', 'HOME_OWNERSHIP', ...]\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('object', ['datetime_as_object']) :  1 | ['EARLIEST_CR_LINE']\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Types of features in processed data (raw dtype, special dtypes):\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('category', [])             :  8 | ['TERM', 'GRADE', 'SUB_GRADE', 'EMP_LENGTH', 'HOME_OWNERSHIP', ...]\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('float', [])                : 17 | ['INT_RATE', 'MNTHS_SINCE_LAST_DELINQ', 'MNTHS_SINCE_LAST_RECORD', 'LOAN_AMNT', 'FUNDED_AMNT', ...]\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('int', [])                  :  1 | ['VERFIED']\u001b[0m\n",
      "\u001b[34mINFO:autogluon.core.features.feature_metadata:#011#011('int', ['datetime_as_int']) :  1 | ['EARLIEST_CR_LINE']\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#0110.9s = Fit runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#01127 features in original data used to generate 27 features in processed data.\u001b[0m\n",
      "\u001b[34mINFO:autogluon.features.generators.abstract:#011Train Data (Processed) Memory Usage: 1.17 MB (0.0% of available memory)\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 0.94s ...\u001b[0m\n",
      "\u001b[34mLevel 25:autogluon.tabular.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#011To change this, specify the eval_metric argument of fit()\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6503, Val Rows: 723\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: RandomForestGini ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8603#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.98s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.11s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: RandomForestEntr ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8631#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0111.04s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.11s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: ExtraTreesGini ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8216#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.74s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.11s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: ExtraTreesEntr ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8257#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.74s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.11s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: KNeighborsUnif ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.7234#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.0s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.2s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: KNeighborsDist ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.7206#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.0s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.19s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: LightGBM ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8672#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.68s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.02s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: LightGBMXT ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8534#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.44s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.02s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: CatBoost ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.87#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0115.36s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.02s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: XGBoost ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8589#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.9s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.02s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: NeuralNetMXNet ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8119#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#01114.24s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.24s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ...\u001b[0m\n",
      "\n",
      "2021-07-30 21:38:03 Uploading - Uploading generated training model\u001b[34mâ–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015â–ˆ#015Epoch 28: early stopping\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8091#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#01118.81s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.19s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: LightGBMLarge ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8672#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.99s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.02s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ...\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.8755#011 = Validation accuracy score\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.66s#011 = Training runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.trainer.abstract_trainer:#0110.0s#011 = Validation runtime\u001b[0m\n",
      "\u001b[34mINFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 50.21s ...\u001b[0m\n",
      "\u001b[34mWARNING:autogluon.core.models.abstract.abstract_model:Deleting model RandomForestEntr. All files under /opt/ml/model/models/RandomForestEntr/ will be removed.\u001b[0m\n",
      "\u001b[34mWARNING:autogluon.core.models.abstract.abstract_model:Deleting model ExtraTreesGini. All files under /opt/ml/model/models/ExtraTreesGini/ will be removed.\u001b[0m\n",
      "\u001b[34mWARNING:autogluon.core.models.abstract.abstract_model:Deleting model ExtraTreesEntr. All files under /opt/ml/model/models/ExtraTreesEntr/ will be removed.\u001b[0m\n",
      "\u001b[34mWARNING:autogluon.core.models.abstract.abstract_model:Deleting model KNeighborsUnif. All files under /opt/ml/model/models/KNeighborsUnif/ will be removed.\u001b[0m\n",
      "\u001b[34mWARNING:autogluon.core.models.abstract.abstract_model:Deleting model LightGBMXT. All files under /opt/ml/model/models/LightGBMXT/ will be removed.\u001b[0m\n",
      "\u001b[34mWARNING:autogluon.core.models.abstract.abstract_model:Deleting model NeuralNetFastAI. All files under /opt/ml/model/models/NeuralNetFastAI/ will be removed.\u001b[0m\n",
      "\u001b[34mINFO:root:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/opt/ml/model/\")\u001b[0m\n",
      "\u001b[34mâ–ˆ#015â–ˆ#015â–ˆ#015*** Summary of fit() ***\u001b[0m\n",
      "\u001b[34mEstimated performance of each model:\n",
      "                 model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\u001b[0m\n",
      "\u001b[34m0  WeightedEnsemble_L2   0.875519       0.608864  23.812589                0.002301           0.662806            2       True          8\u001b[0m\n",
      "\u001b[34m1             CatBoost   0.869986       0.016295   5.361066                0.016295           5.361066            1       True          4\u001b[0m\n",
      "\u001b[34m2        LightGBMLarge   0.867220       0.016922   0.991052                0.016922           0.991052            1       True          7\u001b[0m\n",
      "\u001b[34m3             LightGBM   0.867220       0.016936   0.679440                0.016936           0.679440            1       True          3\u001b[0m\n",
      "\u001b[34m4     RandomForestGini   0.860304       0.111378   0.975989                0.111378           0.975989            1       True          1\u001b[0m\n",
      "\u001b[34m5              XGBoost   0.858921       0.017134   0.899527                0.017134           0.899527            1       True          5\u001b[0m\n",
      "\u001b[34m6       NeuralNetMXNet   0.811895       0.235932  14.239606                0.235932          14.239606            1       True          6\u001b[0m\n",
      "\u001b[34m7       KNeighborsDist   0.720609       0.191966   0.003103                0.191966           0.003103            1       True          2\u001b[0m\n",
      "\u001b[34mNumber of models trained: 8\u001b[0m\n",
      "\u001b[34mTypes of models trained:\u001b[0m\n",
      "\u001b[34m{'WeightedEnsembleModel', 'RFModel', 'KNNModel', 'CatBoostModel', 'LGBModel', 'XGBoostModel', 'TabularNeuralNetModel'}\u001b[0m\n",
      "\u001b[34mBagging used: False \u001b[0m\n",
      "\u001b[34mMulti-layer stack-ensembling used: False \u001b[0m\n",
      "\u001b[34mFeature Metadata (Processed):\u001b[0m\n",
      "\u001b[34m(raw dtype, special dtypes):\u001b[0m\n",
      "\u001b[34m('category', [])             :  8 | ['TERM', 'GRADE', 'SUB_GRADE', 'EMP_LENGTH', 'HOME_OWNERSHIP', ...]\u001b[0m\n",
      "\u001b[34m('float', [])                : 17 | ['INT_RATE', 'MNTHS_SINCE_LAST_DELINQ', 'MNTHS_SINCE_LAST_RECORD', 'LOAN_AMNT', 'FUNDED_AMNT', ...]\u001b[0m\n",
      "\u001b[34m('int', [])                  :  1 | ['VERFIED']\u001b[0m\n",
      "\u001b[34m('int', ['datetime_as_int']) :  1 | ['EARLIEST_CR_LINE']\u001b[0m\n",
      "\u001b[34mPlot summary of models saved to file: /opt/ml/model/SummaryOfModels.html\u001b[0m\n",
      "\u001b[34m*** End of fit() summary ***\u001b[0m\n",
      "\u001b[34mWarning: using box for unknown shape rectagle\n",
      "\u001b[0m\n",
      "\u001b[34mModel export summary:\u001b[0m\n",
      "\u001b[34m/opt/ml/model/: ['models', 'learner.pkl', 'SummaryOfModels.html', 'predictor.pkl']\u001b[0m\n",
      "\u001b[34m/opt/ml/model/models: ['XGBoost', 'NeuralNetMXNet', 'RandomForestGini', 'KNeighborsDist', 'LightGBMLarge', 'trainer.pkl', 'WeightedEnsemble_L2', 'CatBoost', 'LightGBM']\u001b[0m\n",
      "\u001b[34m/opt/ml/model directory size: 37M\n",
      "\u001b[0m\n",
      "\u001b[34mElapsed time: 51.321 seconds. Training Completed!\u001b[0m\n",
      "\u001b[34m2021-07-30 21:38:03,045 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-07-30 21:38:31 Completed - Training job completed\n",
      "ProfilerReport-1627680784: NoIssuesFound\n",
      "Training seconds: 185\n",
      "Billable seconds: 185\n"
     ]
    }
   ],
   "source": [
    "autogluon_model.fit({'training': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Review the output generated by the training job. The top performing model generated by Autogluon should, again, be the WeightedEnsemble. Your model's AUC score on the validation set should be <span style=\"color:lightgreen\">**0.875519**</span>. \n",
    "\n",
    "If you had created a baseline model with the previous dataset version, you would have obtained an AUC score around 0.843707. Thus, the data enrichment yielded significant improvements over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 7: Deploy\n",
    "\n",
    "You can serve your predictions in a couple of ways. You could deploy the model as a [real-time hosted endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html) on SageMaker and integrate it with Snowflake as an [External Function](https://docs.snowflake.com/en/sql-reference/external-functions-creating-aws.html). This will enable you to query your predictions in real-time and minimize data staleness.\n",
    "\n",
    "Alternatively, you can pre-calculate your predictions as a transient batch process. In the following section, you will use [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) to do just that. When your use case allows you to pre-calculate predictions, Batch Transform is a good option. Batch Transform design to scale-out and is optimized for throughput while the real-time endpoints are designed for low latency. Generally, Batch Transform is the cost efficient option as you are only charged for the resources used by the transient batch job. You should run your Batch Transform job as part of an automated workflow in production.\n",
    "\n",
    "In the following sections we are going to deploy our model as a batch inference pipeline. The pipeline is designed to consume data from Snowflake, process it using our DataWrangler flow and then pre-calculate predictions using our trained model and Batch Transform. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7.1 Modify your Data Prepartion flow for Inference\n",
    "\n",
    "You are going to use your model to generate predictions and a credit risk score on unseen data. You can re-use your data preparation flow, but you will need to update your data source.\n",
    "\n",
    "* Make a copy of your flow file. In practice, you should also commit this to version control. \n",
    "* Assign **INFERENCE_FLOW_NAME** with the name of your *.flow* file and run the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_FLOW_NAME = \"dp-inference-loan-default-v2.flow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next edit the query for your loan origination data source. We will use the 20% data sample that we held out for the purpose of demonstration. The query is as follows.\n",
    "\n",
    "   **SELECT** </br>\n",
    "     &nbsp; L1.LOAN_ID, </br>\n",
    "     &nbsp; L1.LOAN_AMNT, </br>\n",
    "     &nbsp; L1.FUNDED_AMNT, </br>\n",
    "     &nbsp; L1.TERM, </br>\n",
    "     &nbsp; L1.INT_RATE, </br>\n",
    "     &nbsp; L1.INSTALLMENT, </br>\n",
    "     &nbsp; L1.GRADE, </br>\n",
    "     &nbsp; L1.SUB_GRADE, </br>\n",
    "     &nbsp; L1.EMP_LENGTH, </br>\n",
    "     &nbsp; L1.HOME_OWNERSHIP, </br>\n",
    "     &nbsp; L1.ANNUAL_INC, </br>\n",
    "     &nbsp; L1.VERIFICATION_STATUS, </br>\n",
    "     &nbsp; L1.PYMNT_PLAN, </br>\n",
    "     &nbsp; L1.PURPOSE, </br>\n",
    "     &nbsp; L1.ZIP_SCODE, </br>\n",
    "     &nbsp; L1.DTI, </br>\n",
    "     &nbsp; L1.DELINQ_2YRS, </br>\n",
    "     &nbsp; L1.EARLIEST_CR_LINE, </br>\n",
    "     &nbsp; L1.INQ_LAST_6MON, </br>\n",
    "     &nbsp; L1.MNTHS_SINCE_LAST_DELINQ, </br>\n",
    "     &nbsp; L1.MNTHS_SINCE_LAST_RECORD, </br>\n",
    "     &nbsp; L1.OPEN_ACC, </br>\n",
    "     &nbsp; L1.PUB_REC, </br>\n",
    "     &nbsp; L1.REVOL_BAL, </br>\n",
    "     &nbsp; L1.REVOL_UTIL, </br>\n",
    "     &nbsp; L1.TOTAL_ACC, </br>\n",
    "     &nbsp; L1.INITIAL_LIST_STATUS, </br>\n",
    "     &nbsp; L1.MTHS_SINCE_LAST_MAJOR_DEROG, </br>\n",
    "     &nbsp; L1.POLICY_CODE, </br>\n",
    "     &nbsp; L1.LOAN_DEFAULT, </br>\n",
    "     &nbsp; L1.ISSUE_MONTH </br>\n",
    "    **FROM** ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML **AS** L1 </br>\n",
    "     &nbsp;**LEFT OUTER JOIN**  </br>\n",
    "     &nbsp;(**SELECT** * FROM ML_LENDER_DATA.ML_DATA.LOAN_DATA_ML **sample block (80) REPEATABLE(100)**) **AS** L2 </br>\n",
    "     &nbsp;**ON** L1.LOAN_ID = L2.LOAN_ID </br>\n",
    "    **WHERE** L2.LOAN_ID **IS NULL** </br>\n",
    "\n",
    "The following video demonstrates how to modify your query.\n",
    "\n",
    "![Edit Query](./images/flow-edit-query.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2 Re-export and Re-factor your Flow as a Pipeline\n",
    "\n",
    "Your goal is to deploy a credit-risk scoring pipeline into production. DataWrangler provides the option to deploy your flow as an Amazon SageMaker Pipeline to facilitate this:\n",
    "\n",
    "<img src=\"./images/export-flow-as-pipeline.png\" width=\"40%\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you will need to refactor the exported script. This has been done for you, so all you need to do is find locate the export node-id. Each step in your data flow is a unique node and the export script is dependent on the node that you select for export. The node id should look like the image below.\n",
    "\n",
    "Copy your node id, assign **FLOW_NODE_ID** to this value and run the following cell.\n",
    "\n",
    "<img src=\"./images/export-node-id.png\" width=\"60%\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW_NODE_ID = \"8cdc7d49-29cd-4657-9ea5-0642ca0bde32.default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell if you like to view the refactored script. The exported pipeline script has been refactored such that it runs a Batch Transform job after the data preparation processing job to generate the credit-risk scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env python\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36muuid\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdw\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ProcessingInput, ProcessingOutput\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdataset_definition\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minputs\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Processor\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnetwork\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m NetworkConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msteps\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ProcessingStep\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minputs\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TransformInput\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msteps\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TransformStep\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    ParameterInteger,\n",
      "    ParameterString,\n",
      ")\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpipeline\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\n",
      "\n",
      "__author__ = \u001b[33m\"\u001b[39;49;00m\u001b[33mDylan Tong\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "__credits__ = [\u001b[33m\"\u001b[39;49;00m\u001b[33mDylan Tong\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "__license__ = \u001b[33m\"\u001b[39;49;00m\u001b[33mApache\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "__version__ = \u001b[33m\"\u001b[39;49;00m\u001b[33m0.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "__maintainer__ = \u001b[33m\"\u001b[39;49;00m\u001b[33mDylan Tong\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "__email__ = \u001b[33m\"\u001b[39;49;00m\u001b[33mdylatong@amazon.com\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "__status__ = \u001b[33m\"\u001b[39;49;00m\u001b[33mPrototype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBlueprintFactory\u001b[39;49;00m() :\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, config):\n",
      "        \n",
      "        \u001b[36mself\u001b[39;49;00m.dw_output_name = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_output_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_output_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.dw_output_name :\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMissing configuration for dw_output_name.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[36mself\u001b[39;49;00m.dw_instance_count           = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.dw_instance_type            = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mml.m5.4xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.dw_volume_size_in_gb        = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_volume_size_in_gb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_volume_size_in_gb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34m30\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.dw_output_content_type      = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_output_content_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_output_content_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mCSV\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.dw_enable_network_isolation = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_enable_network_isolation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_enable_network_isolation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[36mself\u001b[39;49;00m.dw_flow_filepath            = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_flow_filepath\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_flow_filepath\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.dw_flow_filename            = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_flow_filename\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_flow_filename\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.dw_flow_filename :\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMissing configuration for dw_flow_filename\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \n",
      "        \u001b[36mself\u001b[39;49;00m.dw_source_bucket            = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mdw_source_bucket\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mdw_source_bucket\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.dw_source_bucket :\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMissing configuration for dw_source_bucket\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                 \n",
      "        \u001b[36mself\u001b[39;49;00m.batch_instance_count        = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_instance_type         = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mml.c5.2xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m   \n",
      "        \u001b[36mself\u001b[39;49;00m.batch_s3_output_uri         = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_s3_output_uri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_s3_output_uri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.batch_s3_output_uri :\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMissing configuration for batch_s3_output_uri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \n",
      "        \u001b[36mself\u001b[39;49;00m.batch_instance_count        = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_instance_type         = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mml.c5.2xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \n",
      "        \n",
      "        \u001b[36mself\u001b[39;49;00m.sm_estimator                = config[\u001b[33m\"\u001b[39;49;00m\u001b[33msm_estimator\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33msm_estimator\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sm_estimator :\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMissing configuration for sm_estimator\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "     \n",
      "        \u001b[36mself\u001b[39;49;00m.wf_instance_count           = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mwf_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mwf_instance_count\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.wf_instance_type            = config[\u001b[33m\"\u001b[39;49;00m\u001b[33mwf_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mwf_instance_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m config \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mml.m5.4xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m       \n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_batch_pipeline\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m) :    \n",
      "                \n",
      "        iam_role = sagemaker.get_execution_role()\n",
      "        region   = boto3.session.Session().region_name\n",
      "\n",
      "        data_sources = []\n",
      "        sess = sagemaker.Session()\n",
      "        flow_export_id = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mtime.strftime(\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m, time.gmtime())}-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mstr(uuid.uuid4())[:8]}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        flow_export_name = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mflow-\u001b[39;49;00m\u001b[33m{flow_export_id}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        s3_output_prefix = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mexport-\u001b[39;49;00m\u001b[33m{flow_export_name}\u001b[39;49;00m\u001b[33m/output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        s3_output_path = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{self.dw_source_bucket}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{s3_output_prefix}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "        processing_job_output = ProcessingOutput(\n",
      "            output_name=\u001b[36mself\u001b[39;49;00m.dw_output_name,\n",
      "            source=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            destination=s3_output_path,\n",
      "            s3_upload_mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mEndOfJob\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        )\n",
      "\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(\u001b[36mself\u001b[39;49;00m.dw_flow_filepath, \u001b[36mself\u001b[39;49;00m.dw_flow_filename)) \u001b[34mas\u001b[39;49;00m f:\n",
      "            flow = json.load(f)\n",
      "\n",
      "        s3_client = boto3.client(\u001b[33m\"\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        s3_client.upload_file(\u001b[36mself\u001b[39;49;00m.dw_flow_filename, \u001b[36mself\u001b[39;49;00m.dw_source_bucket, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mdata_wrangler_flows/\u001b[39;49;00m\u001b[33m{flow_export_name}\u001b[39;49;00m\u001b[33m.flow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        flow_s3_uri = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{self.dw_source_bucket}\u001b[39;49;00m\u001b[33m/data_wrangler_flows/\u001b[39;49;00m\u001b[33m{flow_export_name}\u001b[39;49;00m\u001b[33m.flow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "        flow_input = ProcessingInput(\n",
      "            source=flow_s3_uri,\n",
      "            destination=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/flow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            input_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mflow\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            s3_data_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mS3Prefix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            s3_input_mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mFile\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            s3_data_distribution_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mFullyReplicated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        )\n",
      "\n",
      "        processing_job_name = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mdata-wrangler-flow-processing-\u001b[39;49;00m\u001b[33m{flow_export_id}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \n",
      "        \u001b[37m## This script was exported for version 1.5.3. As it currently stands, there's no generic script\u001b[39;49;00m\n",
      "        \u001b[37m## if this script stops working, you need to export the flow and refactor the code.\u001b[39;49;00m\n",
      "        container_uri        = utils.dw.get_data_wrangler_container_uri(region, \u001b[33m\"\u001b[39;49;00m\u001b[33m1.x\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        container_uri_pinned = utils.dw.get_data_wrangler_container_uri(region, \u001b[33m\"\u001b[39;49;00m\u001b[33m1.5.3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        output_config = {\n",
      "            \u001b[36mself\u001b[39;49;00m.dw_output_name: {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mcontent_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mself\u001b[39;49;00m.dw_output_content_type\n",
      "            }\n",
      "        }\n",
      "\n",
      "        processor = Processor(\n",
      "            role=iam_role,\n",
      "            image_uri=container_uri,\n",
      "            instance_count=\u001b[36mself\u001b[39;49;00m.dw_instance_count,\n",
      "            instance_type=\u001b[36mself\u001b[39;49;00m.dw_instance_type,\n",
      "            volume_size_in_gb=\u001b[36mself\u001b[39;49;00m.dw_volume_size_in_gb,\n",
      "            network_config=NetworkConfig(enable_network_isolation=\u001b[36mself\u001b[39;49;00m.dw_enable_network_isolation),\n",
      "            sagemaker_session=sess\n",
      "        )\n",
      "\n",
      "        data_wrangler_step = ProcessingStep(\n",
      "            name=\u001b[33m\"\u001b[39;49;00m\u001b[33mDataWranglerProcessingStep\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            processor=processor,\n",
      "            inputs=[flow_input] + data_sources, \n",
      "            outputs=[processing_job_output],\n",
      "            job_arguments=[\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--output-config \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mjson.dumps(output_config)}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        )\n",
      "\n",
      "        transformer = \u001b[36mself\u001b[39;49;00m.sm_estimator.transformer(instance_count = \u001b[36mself\u001b[39;49;00m.batch_instance_count,\n",
      "                                                    strategy       =\u001b[33m'\u001b[39;49;00m\u001b[33mSingleRecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                    assemble_with  =\u001b[33m'\u001b[39;49;00m\u001b[33mLine\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                    instance_type  = \u001b[36mself\u001b[39;49;00m.batch_instance_type,\n",
      "                                                    accept         = \u001b[33m'\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                    output_path    = \u001b[36mself\u001b[39;49;00m.batch_s3_output_uri)\n",
      "\n",
      "        transform_step = TransformStep(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mDefaultRiskScores\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                                       transformer=transformer,\n",
      "                                       inputs=TransformInput(data=data_wrangler_step\n",
      "                                                              .properties\n",
      "                                                              .ProcessingOutputConfig\n",
      "                                                              .Outputs[\u001b[36mself\u001b[39;49;00m.dw_output_name].S3Output.S3Uri,\n",
      "                                                              content_type= \u001b[33m'\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "        wf_instance_type = ParameterString(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mInstanceType\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[36mself\u001b[39;49;00m.wf_instance_type)\n",
      "        wf_instance_count = ParameterInteger(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mInstanceCount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default_value=\u001b[36mself\u001b[39;49;00m.wf_instance_count)\n",
      "        pipeline_name = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mpipeline-\u001b[39;49;00m\u001b[33m{flow_export_name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        pipeline_steps = [data_wrangler_step, transform_step]\n",
      "\n",
      "        pipeline = Pipeline(\n",
      "            name=pipeline_name,\n",
      "            parameters=[wf_instance_type, wf_instance_count],\n",
      "            steps=pipeline_steps,\n",
      "            sagemaker_session=sess\n",
      "        )\n",
      "\n",
      "        pipeline.upsert(role_arn=iam_role)\n",
      "        \u001b[34mreturn\u001b[39;49;00m pipeline\n"
     ]
    }
   ],
   "source": [
    "!pygmentize \"./workflow/pipeline.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to execute your batch scoring pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n"
     ]
    }
   ],
   "source": [
    "batch_output_prefix = \"batch/out\"\n",
    "batch_s3_output_uri = f\"s3://{bucket}/{batch_output_prefix}\"\n",
    "\n",
    "config = {\n",
    "    \"dw_output_name\"              : FLOW_NODE_ID,\n",
    "    \"dw_instance_count\"           : 1,\n",
    "    \"dw_instance_type\"            : \"ml.m5.4xlarge\",\n",
    "    \"dw_flow_filepath\"            : \"\",\n",
    "    \"dw_flow_filename\"            : INFERENCE_FLOW_NAME,\n",
    "    \"dw_volume_size_in_gb\"        : 30,\n",
    "    \"dw_output_content_type\"      : \"CSV\",\n",
    "    \"dw_enable_network_isolation\" : False,\n",
    "    \"dw_source_bucket\"            : bucket,\n",
    "    \"batch_instance_type\"         : \"ml.c5.2xlarge\",\n",
    "    \"batch_instance_count\"        : 1,\n",
    "    \"batch_s3_output_uri\"         : batch_s3_output_uri,\n",
    "    \"wf_instance_type\"            : \"ml.m5.4xlarge\",\n",
    "    \"wf_instance_count\"           : 1,\n",
    "    \"sm_estimator\"                : autogluon_model,\n",
    "}\n",
    "\n",
    "bpf      = BlueprintFactory(config)\n",
    "pipeline = bpf.get_batch_pipeline()\n",
    "\n",
    "execution = pipeline.start()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can monitor the status of your pipeline from Amazon SageMaker Studio. The following video demonstrates how to do this.\n",
    "\n",
    "![Pipeline Status](./images/pipeline-status.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit-risk prediction data is small enough for us to load into a local pandas dataframe. Run the following cell to preview the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "output_uri = utils.dw.get_data_uri(batch_s3_output_uri)\n",
    "results = pd.read_csv(output_uri, header=None, names=[\"label\",\"probas\"])\n",
    "\n",
    "results[[\"p_default\"]] = pd.DataFrame(results[\"probas\"].str[1:-1].str.split(\",\", expand=True)[1].astype(float))\n",
    "results[\"predictions\"] = (results[\"p_default\"] > threshold).astype(int)\n",
    "\n",
    "cols            = [\"label\",\"predictions\",\"p_default\"] \n",
    "results_file    = \"results.csv\"\n",
    "results_prefix  = \"results\"\n",
    "\n",
    "results.to_csv(path_or_buf=results_file, columns = cols, index=False)\n",
    "S3Uploader.upload(results_file, f\"s3://{bucket}/{results_prefix}\")\n",
    "\n",
    "results[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our hold-out test set to generate the scores. Let's evaluate our model performance with the provided utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector_params = {\n",
    "    \"workspace\": bucket,\n",
    "    \"drivers\":{\n",
    "        \"db\": boto3.client(\"s3\"),\n",
    "        \"dsmlp\": boto3.client(\"sagemaker\"),\n",
    "    },\n",
    "    \"prefixes\": {\n",
    "        \"results_path\": results_prefix,\n",
    "        \"bias_path\": None,\n",
    "        \"xai_path\": None,\n",
    "    },\n",
    "    \"results-config\":{\n",
    "        \"gt_index\": 0,\n",
    "        \"pred_index\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "inspector = ModelInspector.get_inspector(inspector_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "inspector.display_interactive_cm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lastly, we're going to use the Snowflake Python connector to load our predictions into Snowflake to drive credit-risk analysis.\n",
    "\n",
    "In practice, the process of loading the predictions from S3 into Snowflake should be part of your production scoring pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "# Connecting to Snowflake using the default authenticator\n",
    "\n",
    "#TODO, SNOWFLAKE TEAM WILL CREATE SCRIPTS\n",
    "ctx = snowflake.connector.connect(\n",
    "  user='xxxxxxxx',\n",
    "  password='xxxxxxxx',\n",
    "  account='xxxxxxx',\n",
    "  warehouse='xxxxxxx',\n",
    "  database='xxxxxxx',\n",
    "  schema='PUBLIC'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "# Write the predictions to the table named \"ML_RESULTS\".\n",
    "success, nchunks, nrows, _ = write_pandas(ctx, results[cols], 'ML_RESULTS')\n",
    "\n",
    "display(nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up\n",
    "\n",
    "Congratulations! You've completed the lab. You can delete the active resources created for the lab by deleting the CloudFormation template.\n",
    "\n",
    "You can also remove the database resources that were created with the following commands.\n",
    "\n",
    "DROP STORAGE INTEGRATION sagemaker_datawrangler_integration  </br>\n",
    "REVOKE CREATE stage ON SCHEMA public FROM role sagemaker_role  </br>\n",
    "DROP stage datawrangler_stage  </br>\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
